{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Find the most visited URL per country per day during a week of your choice (e.g. 2018-04-01 until 2018-04-08)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "As part of this challenge, we would like you to create a more detailed diagram of the whole system, with every resource reflected on it. Include an explanation on what it is that it does and your approach to solving the challenge.\n",
    "\n",
    "Feel free to include as well any code or screenshots that are part of your solution if relevant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "#### Athena\n",
    "1. Understand the data using Athena i.e. what the column names mean, what kind of values and data types\n",
    "2. Choose which columns would be useful - i.e. country_code, url_visited, date\n",
    "3. Test queries out step by step with the following to try out:\n",
    "    * COUNT(date) to count how many url per country per day\n",
    "    * Filter the dates: BETWEEN 2018-04-01 AND 2018-04-08\n",
    "    * Use GROUP by on country_code, date\n",
    "    * Do some research to find out how we could get the url that is most visited on the url column\n",
    "4. Repeat testing until the query performs as expected and gives the output I am looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the table I expect to transform to be loaded to my psql table locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| country_code | date       | most_visited_url                |\n",
    "|---------|------------|--------------------|\n",
    "| US    | 2018-04-01 | www.netflix.com |\n",
    "| CA   | 2018-04-01 | www.netflix.com  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data and optimising my SQL queries in Athena\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the first five rows to understand what kind of values each column contains\n",
    "```\n",
    "SELECT *\n",
    "FROM vod_clickstream\n",
    "LIMIT 5;\n",
    "```\n",
    "* After executing this query, I found that \n",
    "    * `server_request_country_code` shows the country code\n",
    "    * `dt` shows the full date i.e. 2016-06-09\n",
    "    * `event_url` shows the url visited i.e. https://www.netflix.com/browse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Base on the above findings, I decided to narrow down and check out these three columns\n",
    "```\n",
    "SELECT \n",
    "server_request_country_code, \n",
    "dt, \n",
    "event_url\n",
    "FROM \n",
    "vod_clickstream\n",
    "LIMIT 5;\n",
    "```\n",
    "Below is the output I got from Athena and I thought it looked good with the info I need find out what's the most visited url for each country per day for a week of my choice\n",
    "![Image showing the output from Athena after narrowing down to the 3 columns](./athena_outputs/narrow_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After this I decided to try and filter the dates using the BETWEEN clause\n",
    "```\n",
    "SELECT \n",
    "server_request_country_code, \n",
    "dt, \n",
    "event_url\n",
    "FROM \n",
    "vod_clickstream\n",
    "WHERE dt BETWEEN '2018-04-01' AND '2018-04-08'\n",
    "LIMIT 5;\n",
    "```\n",
    "![Image showing the output from Athena after  filtering on the dates to the week of my choice](./athena_outputs/filter_dates_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next I decided to try group by on the country code and date so that I can see if it includes all the dates I need. I choose to filter on the country code to be Hong Kong so it's easier to see without all the countries. In the process I was reminded that to use GROUP BY you need to have an aggregate function and I decided to use COUNT on the event_url to count the number of urls visited.\n",
    "```\n",
    "SELECT \n",
    "server_request_country_code,  \n",
    "dt, \n",
    "COUNT(event_url) AS total_url_visited\n",
    "FROM \n",
    "vod_clickstream\n",
    "WHERE dt BETWEEN '2018-04-01' AND '2018-04-08'\n",
    "AND server_request_country_code = 'HK'\n",
    "GROUP BY server_request_country_code, dt;\n",
    "```\n",
    "![Image showing the output of the GROUP BY query filtering on country being Hong Kong](./athena_outputs/group_by_check_dates_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great except we need to filter out one of the extra days as there are 8 days above. After further checking, 2018-04-01 is a Sunday and I want to use the week beginning on a Monday so this date will be filtered out now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Adjusted the BETWEEN clause so 2018-04-01 is not included\n",
    "```\n",
    "SELECT \n",
    "server_request_country_code,  \n",
    "dt, \n",
    "COUNT(event_url) AS total_url_visited\n",
    "FROM \n",
    "vod_clickstream\n",
    "WHERE dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "AND server_request_country_code = 'HK'\n",
    "GROUP BY server_request_country_code, dt;\n",
    "```\n",
    "![Image showing 2018-04-01 not included in the results anymore](./athena_outputs/optimised_between_clause_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research, I found out that I just need to add the url column on to the SELECT and GROUP BY clause so that it uniquely identifies the country code, date and url combinations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Adjusted the SQL query and aded `event_url` in the SELECT and GROUP BY clause to see if it can identify unique combinations of country code, date and url for Hong Kong. Also decided to rename the ALIAS on the count to `total_visited`\n",
    "```\n",
    "SELECT \n",
    "server_request_country_code,  \n",
    "dt, \n",
    "event_url,\n",
    "COUNT(*) AS total_visited\n",
    "FROM \n",
    "vod_clickstream\n",
    "WHERE dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "AND server_request_country_code = 'HK'\n",
    "GROUP BY server_request_country_code, dt, event_url;\n",
    "```\n",
    "This gave me 6000+ rows and I could definitely see unique combinations of country code, date and url\n",
    "![Image showing 15 rows of the sql statement used above](./athena_outputs/unique_combinations_query.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. After some research I think the above query can be used as an inner query and we need an outer query to find the most frequent url. What I think we need to do is SELECT MAX on the `event_url` and `total_visited` columns. Then use a GROUP BY clause on the country code and date.\n",
    "```\n",
    "SELECT\n",
    "    server_request_country_code,\n",
    "    dt,\n",
    "    MAX(event_url) AS most_visited_url,\n",
    "    MAX(total_visited) AS total_visited\n",
    "FROM (\n",
    "    SELECT \n",
    "        server_request_country_code,  \n",
    "        dt, \n",
    "        event_url,\n",
    "        COUNT(*) AS total_visited\n",
    "    FROM \n",
    "        vod_clickstream\n",
    "    WHERE \n",
    "        dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "        AND server_request_country_code = 'HK'\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt, event_url\n",
    ") AS url_counts\n",
    "GROUP BY \n",
    "    server_request_country_code, dt;\n",
    "```\n",
    "![Image showing the result after adding MAX in the outer query and using the old query as the inner query](./athena_outputs/using_max_and_subquery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks really good, we will just need to work on how to ORDER it in the final query. Let's optimise this query with ORDER by first on the country code and date before getting rid of Hong Kong in the filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ADD ORDER BY at the end for country code and date (only date is really needed here but using both to check it works before removing HK)\n",
    "```\n",
    "SELECT\n",
    "    server_request_country_code,\n",
    "    dt,\n",
    "    MAX(event_url) AS most_visited_url,\n",
    "    MAX(total_visited) AS total_visited\n",
    "FROM (\n",
    "    SELECT \n",
    "        server_request_country_code,  \n",
    "        dt, \n",
    "        event_url,\n",
    "        COUNT(*) AS total_visited\n",
    "    FROM \n",
    "        vod_clickstream\n",
    "    WHERE \n",
    "        dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "        AND server_request_country_code = 'HK'\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt, event_url\n",
    ") AS url_counts\n",
    "GROUP BY \n",
    "    server_request_country_code, dt\n",
    "ORDER BY\n",
    "    server_request_country_code, dt;\n",
    "```\n",
    "![Image showing optimised query output ordering by date](./athena_outputs/order_by_date_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great, we can it's showing in order by the date. Now let's get rid of HK in the inner query filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Removed filter on country code being Hong Kong and I decided to keep the `total_visited` as I think it's useful although not asked by the challenge and not what I had in mind originally from my plan. \n",
    "```\n",
    "SELECT\n",
    "    server_request_country_code,\n",
    "    dt,\n",
    "    MAX(event_url) AS most_visited_url,\n",
    "    MAX(total_visited) AS total_visited\n",
    "FROM (\n",
    "    SELECT \n",
    "        server_request_country_code,  \n",
    "        dt, \n",
    "        event_url,\n",
    "        COUNT(*) AS total_visited\n",
    "    FROM \n",
    "        vod_clickstream\n",
    "    WHERE \n",
    "        dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt, event_url\n",
    ") AS url_counts\n",
    "GROUP BY \n",
    "    server_request_country_code, dt\n",
    "ORDER BY\n",
    "    server_request_country_code, dt;\n",
    "```\n",
    "![Image showing all countries with the most frequently visited link](./athena_outputs/removed_hk_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are questionable data here where I first see some empty values for country code in the first 6 rows and also A1 and A2. I would prefer to get rid of the rows where the country code is NULL first in the inner query in this case. Also have a look at what A1 and A2 country code is as in my domain knowledge, that doesn't look like a country code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so after some research I found that A1 country code stands for `Anonymous proxy`. What Anonymous proxies are is that it allows users to hide where their traffic is coming from (IP Address) so we don't know which country they are coming from. Let's drop the empty country code and the A1 country code. I couldn't find any information on A2 but as in my domain it doesn't look like a country code, I think it's best to drop this out of the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Remove empty string, A1 and A2 through inner query by adding country code `NOT IN ('A1', 'A2', '')`\n",
    "```\n",
    "SELECT\n",
    "    server_request_country_code,\n",
    "    dt,\n",
    "    MAX(event_url) AS most_visited_url,\n",
    "    MAX(total_visited) AS total_visited\n",
    "FROM (\n",
    "    SELECT \n",
    "        server_request_country_code,  \n",
    "        dt, \n",
    "        event_url,\n",
    "        COUNT(*) AS total_visited\n",
    "    FROM \n",
    "        vod_clickstream\n",
    "    WHERE \n",
    "        dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "        AND server_request_country_code NOT IN ('A1', 'A2', '')\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt, event_url\n",
    ") AS url_counts\n",
    "GROUP BY \n",
    "    server_request_country_code, dt\n",
    "ORDER BY\n",
    "    server_request_country_code, dt;\n",
    "```\n",
    "![Image showing optimised output and getting rid of rows that are not real country codes](./athena_outputs/filter_out_not_real_country_code_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought this was all good but then realised the MAX clause when used on strings returns in lexicographical order, which is the order in which words are listed in the dictionary (A-Z) - thanks to this [article](https://dbschema.com/2023/08/25/sql-tutorial/sql-min-max-functions/#:~:text=MAX()%20and%20MIN()%20with%20strings,highest%20value%20in%20the%20column.). So in this case that doesn't return the most frequent string so I decided to do more research and see what is the right method to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research and coming across this medium [article](https://medium.com/@pradeepchandrareddy95/how-to-calculate-mode-in-sql-how-to-find-most-frequent-value-in-a-column-b3f913fdeb1e), I think using CTE would be a good way to get the mode of the `event_url`. I found a good example of how to do this:\n",
    "```\n",
    "WITH CategoryCounts AS (\n",
    "    SELECT product_category, COUNT(*) AS category_count\n",
    "    FROM sales\n",
    "    GROUP BY product_category\n",
    ")\n",
    "\n",
    "SELECT product_category, category_count\n",
    "FROM CategoryCounts\n",
    "WHERE category_count = (\n",
    "    SELECT MAX(category_count)\n",
    "    FROM CategoryCounts\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTE allows you to make temporary tables and store results into this and then use SELECT, INSERT, UPDATE statements. I think it would be good to make a temporary table with the inner query I have and write a new SELECT statement witha subquery inside like the one above to do MAX on the `total_visited`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "-- Make a CTE to count the visits per URL and country within the date range I choose and exclude unwanted country codes\n",
    "WITH UrlCounts AS (\n",
    "    SELECT \n",
    "        server_request_country_code,  \n",
    "        dt, \n",
    "        event_url,\n",
    "        COUNT(*) AS total_visited\n",
    "    FROM \n",
    "        vod_clickstream\n",
    "    WHERE \n",
    "        dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "        AND server_request_country_code NOT IN ('A1', 'A2', '')\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt, event_url\n",
    ")\n",
    "\n",
    "-- Select the country, date, and URL with the maximum number of visits, \n",
    "-- Order by country code and date\n",
    "SELECT \n",
    "    server_request_country_code,\n",
    "    dt,\n",
    "    event_url,\n",
    "    total_visited\n",
    "FROM \n",
    "    UrlCounts\n",
    "WHERE (server_request_country_code, dt, total_visited) IN (\n",
    "    SELECT \n",
    "        server_request_country_code,\n",
    "        dt,\n",
    "        MAX(total_visited) AS max_visited\n",
    "    FROM UrlCounts\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt\n",
    ") \n",
    "ORDER BY server_request_country_code, dt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image showing final query after using CTE to make a temporary table](./athena_outputs/cte_and_subquery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `total_visited` column gives visibility as to how many counts there were for the most visited url and also we can see in the above that the query is working as expected given that **the second and third row of AD FOR '2018-04-03' have two different most frequently visited links both with a count of 7**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great now, we are ready to transform and load the data to a psql table locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import psycopg2   \n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sql query\n",
    "sql_query = \"\"\"\n",
    "WITH UrlCounts AS (\n",
    "    SELECT \n",
    "        server_request_country_code,  \n",
    "        dt, \n",
    "        event_url,\n",
    "        COUNT(*) AS total_visited\n",
    "    FROM \n",
    "        vod_clickstream\n",
    "    WHERE \n",
    "        dt BETWEEN '2018-04-02' AND '2018-04-08'\n",
    "        AND server_request_country_code NOT IN ('A1', 'A2', '')\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt, event_url\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    server_request_country_code,\n",
    "    dt,\n",
    "    event_url,\n",
    "    total_visited\n",
    "FROM \n",
    "    UrlCounts\n",
    "WHERE (server_request_country_code, dt, total_visited) IN (\n",
    "    SELECT \n",
    "        server_request_country_code,\n",
    "        dt,\n",
    "        MAX(total_visited) AS max_visited\n",
    "    FROM UrlCounts\n",
    "    GROUP BY \n",
    "        server_request_country_code, dt\n",
    ") \n",
    "ORDER BY server_request_country_code, dt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aws credentials\n",
    "load_dotenv()\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Athena client\n",
    "athena_client = boto3.client(\n",
    "    'athena',\n",
    "    region_name='eu-west-2',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a sql query in Athena and save the info about the query in query_execution\n",
    "query_execution = athena_client.start_query_execution(\n",
    "    QueryString=sql_query,\n",
    "    QueryExecutionContext={\n",
    "        \"Database\": \"denise-athena-parquet\" # Set the database it will run on\n",
    "    },\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": \"s3://athena-learners-etl-bite05/denise/\" # Set where the query results will go to\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the query status variable to \"QUEUED\"\n",
    "query_status = \"QUEUED\"\n",
    "# Get the query id of the query executed above \n",
    "query_execution_id = query_execution[\"QueryExecutionId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enters into a while loop whilst the query_status = \"QUEUED\" or \"RUNNING\"\n",
    "while query_status in [\"QUEUED\", \"RUNNING\"]:\n",
    "    # Get info about the query and saved to variable query_execution \n",
    "    query_execution = athena_client.get_query_execution(\n",
    "        QueryExecutionId=query_execution_id\n",
    "    )\n",
    "    # Retrieve the query status\n",
    "    query_status = query_execution[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "    # Checks if the query has failed, if so raise exception\n",
    "    if query_status == \"FAILED\":\n",
    "        raise Exception(\"Athena query failed!\")\n",
    "    # Tell athena to wait to avoid overwhelming athena\n",
    "    time.sleep(1)\n",
    "\n",
    "# Retrieve the query results\n",
    "results = athena_client.get_query_results(\n",
    "    QueryExecutionId=query_execution_id\n",
    ")[\"ResultSet\"][\"Rows\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the local Postgres database\n",
    "conn = psycopg2.connect(database=\"etl_bites\", user=\"denisechan\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table if it exists, otherwise create it\n",
    "cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS most_frequent_url_per_country_per_day;\n",
    "    -- Create the table\n",
    "    CREATE TABLE most_frequent_url_per_country_per_day (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        country_code VARCHAR(2),\n",
    "        date DATE,\n",
    "        most_visited_url VARCHAR(255),\n",
    "        total_visited INTEGER\n",
    "    );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the query results\n",
    "for row in results[1:]:\n",
    "    # Checks if the three columns has empty values\n",
    "    if not row[\"Data\"][0] or not row[\"Data\"][1] or not row[\"Data\"][2]:\n",
    "        # Prints the whole row if the first two values are empty\n",
    "        print(f\"Skipping row: {row}\")\n",
    "        continue\n",
    "\n",
    "    country_code = row[\"Data\"][0][\"VarCharValue\"]\n",
    "    date = row[\"Data\"][1][\"VarCharValue\"]\n",
    "    most_visited_url = row[\"Data\"][2][\"VarCharValue\"]\n",
    "    total_visited = int(row[\"Data\"][3][\"VarCharValue\"])\n",
    "\n",
    "    # Insert the data into the local PostgreSQL database\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO most_frequent_url_per_country_per_day (country_code, date, most_visited_url, total_visited)\n",
    "        VALUES (%s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(insert_query, (country_code, date, most_visited_url, total_visited))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred inserting into analytical DB: %s\"% e)\n",
    "        conn.rollback()  # Rollback the transaction if there's an error\n",
    "\n",
    "# Commit the changes and close the cursor and connection outside the loop\n",
    "conn.commit() # Commits the changes to the database\n",
    "cursor.close() # Closes the cursor\n",
    "conn.close() # Close the database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data has been loaded in my psql table locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql+psycopg2://localhost:5432/etl_bites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the rows for `country_code = AD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://localhost:5432/etl_bites\n",
      "8 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th>\n",
       "            <th>country_code</th>\n",
       "            <th>date</th>\n",
       "            <th>most_visited_url</th>\n",
       "            <th>total_visited</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-02</td>\n",
       "            <td>https://www.netflix.com/browse</td>\n",
       "            <td>9</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-03</td>\n",
       "            <td>https://www.netflix.com/browse</td>\n",
       "            <td>7</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-03</td>\n",
       "            <td>https://www.netflix.com/ad/login</td>\n",
       "            <td>7</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-04</td>\n",
       "            <td>https://www.netflix.com/browse</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-05</td>\n",
       "            <td>https://www.netflix.com/browse</td>\n",
       "            <td>4</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-06</td>\n",
       "            <td>https://www.netflix.com/browse</td>\n",
       "            <td>4</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-07</td>\n",
       "            <td>https://www.netflix.com/browse</td>\n",
       "            <td>12</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8</td>\n",
       "            <td>AD</td>\n",
       "            <td>2018-04-08</td>\n",
       "            <td>https://www.netflix.com/browse</td>\n",
       "            <td>7</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 'AD', datetime.date(2018, 4, 2), 'https://www.netflix.com/browse', 9),\n",
       " (2, 'AD', datetime.date(2018, 4, 3), 'https://www.netflix.com/browse', 7),\n",
       " (3, 'AD', datetime.date(2018, 4, 3), 'https://www.netflix.com/ad/login', 7),\n",
       " (4, 'AD', datetime.date(2018, 4, 4), 'https://www.netflix.com/browse', 5),\n",
       " (5, 'AD', datetime.date(2018, 4, 5), 'https://www.netflix.com/browse', 4),\n",
       " (6, 'AD', datetime.date(2018, 4, 6), 'https://www.netflix.com/browse', 4),\n",
       " (7, 'AD', datetime.date(2018, 4, 7), 'https://www.netflix.com/browse', 12),\n",
       " (8, 'AD', datetime.date(2018, 4, 8), 'https://www.netflix.com/browse', 7)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM most_frequent_url_per_country_per_day \n",
    "LIMIT 8;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl_cloud_db_challenge_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
